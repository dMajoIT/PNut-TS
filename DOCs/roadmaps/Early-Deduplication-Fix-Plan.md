# Early Object Deduplication Fix Plan

## Problem Analysis

### Current Situation
The PNut-TS compiler has a **disabled early deduplication pass** intended to reduce memory usage during compilation by detecting duplicate child objects before they are stored. This optimization was disabled due to crashes caused by implementation issues.

**Location**: `src/classes/compiler.ts:241-242`
```typescript
const childExists: boolean = false; //this.childImages.isChildPresent(childImage);
//const childExists: boolean = this.childImages.isChildPresent(childImage);
```

### Why Early Deduplication Matters
1. **Memory Pressure Reduction**: Prevents storing duplicate object binaries during compilation
2. **Build Performance**: Reduces memory allocation and copying overhead
3. **Scalability**: Enables compilation of larger object trees without hitting memory limits
4. **Compiler Efficiency**: Complements the later distiller optimization

### Root Cause Analysis

#### Problem 1: Index Corruption
**Issue**: The `isChildPresent()` method correctly identifies duplicates but **doesn't return the matching file index**. When a duplicate is found, the code still:
1. Increments `objectFileCount`
2. Increments `objectFileOffset`
3. Records a new file entry

This creates **index misalignment** between:
- `objectCountsPerChild[]` array (expects sequential indices)
- `childImages` file tracking (has gaps due to skipped duplicates)

#### Problem 2: Reference Tracking Inconsistency
**Issue**: Later code in `compileRecursively()` uses `objectCountsPerChild[childIdx]` to access child objects:
```typescript
const fileIdx = objectCountsPerChild[childIdx];
const [objOffset, objLength] = this.childImages.getOffsetAndLengthForFile(fileIdx);
```

When duplicates are skipped, `fileIdx` becomes invalid, causing:
- Invalid array access
- Incorrect object data retrieval
- Compiler crashes or corrupted binaries

#### Problem 3: Binary Content vs. File Identity Confusion
**Issue**: The current implementation compares **binary content** but doesn't account for the fact that identical objects may need **different references** in the parent object.

### Example Failure Scenario
```
Object Tree:
├── main.spin2
├── child1.spin2 (identical to child2.spin2)
├── child2.spin2 (identical to child1.spin2)
└── child3.spin2

Compilation Flow:
1. Compile child1.spin2 → objectFileCount=0, stored at fileIdx=0
2. Compile child2.spin2 → isChildPresent()=true, SKIPPED
   - objectFileCount=0 (not incremented) ← PROBLEM
   - objectCountsPerChild=[0, ???] ← BROKEN
3. Later: access objectCountsPerChild[1] → undefined fileIdx
4. Call getOffsetAndLengthForFile(undefined) → CRASH
```

## Comprehensive Fix Plan

### Phase 1: Enhanced Duplicate Detection with Index Mapping

#### Step 1.1: Modify `isChildPresent()` to Return Match Index
```typescript
// New method signature in ChildObjectsImage
public findDuplicateChild(childImage: Uint8Array): { exists: boolean; fileIndex: number } {
  for (let fileIdx = 0; fileIdx < this.objectFileCount; fileIdx++) {
    const [objOffset, objLength] = this.getOffsetAndLengthForFile(fileIdx);
    if (childImage.length === objLength) {
      const possibleChildImage = this.rawUint8Array.subarray(objOffset, objOffset + objLength);
      const sameChild = possibleChildImage.every((byte, idx) => byte === childImage[idx]);
      if (sameChild) {
        return { exists: true, fileIndex: fileIdx };
      }
    }
  }
  return { exists: false, fileIndex: -1 };
}
```

#### Step 1.2: Create Index Mapping System
```typescript
// In Compiler class - add new properties
private childObjectIndexMap: Map<number, number> = new Map(); // logicalIndex -> physicalIndex
private nextLogicalIndex: number = 0;

// Modified compilation logic
const duplicateInfo = this.childImages.findDuplicateChild(childImage);
let physicalFileIndex: number;

if (duplicateInfo.exists) {
  // Reuse existing object
  physicalFileIndex = duplicateInfo.fileIndex;
  this.logMessageOutline(`  -- REUSE DUPE -- logicalIdx=(${this.nextLogicalIndex}), physicalIdx=(${physicalFileIndex})`);
} else {
  // Store new object
  physicalFileIndex = this.objectFileCount;
  this.childImages.setOffset(this.objectFileOffset);
  this.childImages.ensureFits(this.objectFileOffset, objectLength);
  this.childImages.rawUint8Array.set(childImage, this.objectFileOffset);
  this.childImages.recordLengthOffsetForFile(this.objectFileCount, this.objectFileOffset, objectLength);

  this.objectFileOffset += objectLength;
  this.objectFileCount++;
  this.logMessageOutline(`  -- NEW OBJECT -- logicalIdx=(${this.nextLogicalIndex}), physicalIdx=(${physicalFileIndex})`);
}

// Map logical index to physical index
this.childObjectIndexMap.set(this.nextLogicalIndex, physicalFileIndex);
objectCountsPerChild.push(this.nextLogicalIndex); // Use logical index
this.nextLogicalIndex++;
```

#### Step 1.3: Update Object Access Logic
```typescript
// Modified object access in compileRecursively()
for (let childIdx = 0; childIdx < objectFiles; childIdx++) {
  const logicalFileIdx = objectCountsPerChild[childIdx];
  const physicalFileIdx = this.childObjectIndexMap.get(logicalFileIdx);

  if (physicalFileIdx === undefined) {
    throw new Error(`Internal error: missing index mapping for logical index ${logicalFileIdx}`);
  }

  const [objOffset, objLength] = this.childImages.getOffsetAndLengthForFile(physicalFileIdx);
  // ... rest of processing
}
```

### Phase 2: Memory Usage Tracking

#### Step 2.1: Add Memory Statistics
```typescript
// Add to Compiler class
private memoryStats = {
  totalObjectsCompiled: 0,
  duplicatesDetected: 0,
  memoryBytesSaved: 0,
  duplicatesBySize: new Map<number, number>()
};

// Track savings when duplicate found
if (duplicateInfo.exists) {
  this.memoryStats.duplicatesDetected++;
  this.memoryStats.memoryBytesSaved += objectLength;

  const sizeCount = this.memoryStats.duplicatesBySize.get(objectLength) || 0;
  this.memoryStats.duplicatesBySize.set(objectLength, sizeCount + 1);
}
```

#### Step 2.2: Add Logging and Diagnostics
```typescript
// Enhanced logging for duplicate detection
private logDuplicationStats(): void {
  if (this.memoryStats.duplicatesDetected > 0) {
    this.logMessageOutline(`++ EARLY DEDUPLICATION STATS:`);
    this.logMessageOutline(`   Objects compiled: ${this.memoryStats.totalObjectsCompiled}`);
    this.logMessageOutline(`   Duplicates found: ${this.memoryStats.duplicatesDetected}`);
    this.logMessageOutline(`   Memory saved: ${this.memoryStats.memoryBytesSaved} bytes`);
    this.logMessageOutline(`   Dedup ratio: ${(this.memoryStats.duplicatesDetected / this.memoryStats.totalObjectsCompiled * 100).toFixed(1)}%`);
  }
}
```

### Phase 3: Safety and Validation

#### Step 3.1: Add Comprehensive Validation
```typescript
// Validation method to ensure consistency
private validateChildObjectIndices(): void {
  // Verify all mappings are valid
  for (const [logicalIdx, physicalIdx] of this.childObjectIndexMap) {
    if (physicalIdx >= this.objectFileCount) {
      throw new Error(`Invalid mapping: logical ${logicalIdx} -> physical ${physicalIdx} (max: ${this.objectFileCount - 1})`);
    }
  }

  // Verify no gaps in logical indices
  const expectedLogicalIndices = Array.from(this.childObjectIndexMap.keys()).sort((a, b) => a - b);
  for (let i = 0; i < expectedLogicalIndices.length; i++) {
    if (expectedLogicalIndices[i] !== i) {
      throw new Error(`Gap in logical indices: expected ${i}, found ${expectedLogicalIndices[i]}`);
    }
  }
}
```

#### Step 3.2: Add Binary Comparison Optimization
```typescript
// Optimized binary comparison for large objects
private compareChildImages(image1: Uint8Array, image2: Uint8Array): boolean {
  if (image1.length !== image2.length) return false;

  // Quick hash comparison for large objects
  if (image1.length > 1024) {
    const hash1 = this.quickHash(image1);
    const hash2 = this.quickHash(image2);
    if (hash1 !== hash2) return false;
  }

  // Byte-by-byte comparison
  return image1.every((byte, idx) => byte === image2[idx]);
}

private quickHash(data: Uint8Array): number {
  let hash = 0;
  const step = Math.max(1, Math.floor(data.length / 64)); // Sample every N bytes
  for (let i = 0; i < data.length; i += step) {
    hash = ((hash << 5) - hash + data[i]) | 0; // Simple hash function
  }
  return hash;
}
```

### Phase 4: Integration and Testing

#### Step 4.1: Gradual Rollout Strategy
```typescript
// Add feature flag for early deduplication
export interface CompileOptions {
  // ... existing options
  enableEarlyDeduplication: boolean; // New flag
}

// Modified compilation logic with feature flag
const duplicateInfo = this.context.compileOptions.enableEarlyDeduplication
  ? this.childImages.findDuplicateChild(childImage)
  : { exists: false, fileIndex: -1 };
```

#### Step 4.2: Test Cases
1. **Basic Deduplication**: Objects with identical binary content
2. **Index Mapping**: Verify logical-to-physical index translation
3. **Memory Limits**: Large object trees with many duplicates
4. **Edge Cases**: Single child, all duplicates, no duplicates
5. **Performance**: Compare with/without early deduplication

### Phase 5: Performance Optimization

#### Step 5.1: Efficient Duplicate Detection
```typescript
// Add content hash cache to avoid repeated comparisons
private contentHashCache: Map<string, number> = new Map();

private getContentHash(childImage: Uint8Array): string {
  // Use first/last 32 bytes + length as quick signature
  const signature = new Uint8Array(68); // 32 + 32 + 4
  signature.set(childImage.subarray(0, Math.min(32, childImage.length)), 0);
  signature.set(childImage.subarray(Math.max(0, childImage.length - 32)), 32);

  // Add length as 4-byte value
  const lengthBytes = new Uint8Array(4);
  new DataView(lengthBytes.buffer).setUint32(0, childImage.length, true);
  signature.set(lengthBytes, 64);

  return Array.from(signature).map(b => b.toString(16).padStart(2, '0')).join('');
}
```

#### Step 5.2: Lazy Binary Comparison
```typescript
// Only do expensive byte-by-byte comparison when hash matches
public findDuplicateChildOptimized(childImage: Uint8Array): { exists: boolean; fileIndex: number } {
  const childHash = this.getContentHash(childImage);

  // Check if we've seen this content signature before
  if (this.contentHashCache.has(childHash)) {
    const candidateIndex = this.contentHashCache.get(childHash)!;
    const [objOffset, objLength] = this.getOffsetAndLengthForFile(candidateIndex);

    if (objLength === childImage.length) {
      const existingImage = this.rawUint8Array.subarray(objOffset, objOffset + objLength);
      if (this.compareChildImages(existingImage, childImage)) {
        return { exists: true, fileIndex: candidateIndex };
      }
    }
  }

  return { exists: false, fileIndex: -1 };
}
```

## Implementation Timeline

### Phase 1: Core Fix (8-12 hours)
- Implement `findDuplicateChild()` method
- Add index mapping system
- Update object access logic
- Basic validation

### Phase 2: Memory Tracking (4-6 hours)
- Add memory statistics
- Implement logging and diagnostics
- Performance monitoring

### Phase 3: Safety Features (6-8 hours)
- Comprehensive validation
- Binary comparison optimization
- Error handling and recovery

### Phase 4: Testing & Integration (8-12 hours)
- Unit tests for deduplication logic
- Integration tests with various object trees
- Performance benchmarking
- Feature flag implementation

### Phase 5: Optimization (4-6 hours)
- Hash-based duplicate detection
- Lazy binary comparison
- Memory usage optimization

**Total Estimated Effort**: 30-44 hours

## Success Criteria

### Functional Requirements
- [ ] Early deduplication pass runs without crashes
- [ ] Binary output remains identical to current implementation (when disabled)
- [ ] Significant memory usage reduction with complex object trees
- [ ] All regression tests pass

### Performance Requirements
- [ ] No performance degradation when feature disabled
- [ ] <5% compilation time increase when feature enabled
- [ ] 20-50% memory usage reduction for duplicate-heavy projects

### Quality Requirements
- [ ] Comprehensive test coverage for edge cases
- [ ] Clear logging and diagnostic information
- [ ] Graceful handling of memory pressure
- [ ] Feature flag for safe rollout

## Benefits After Implementation

### Immediate Benefits
- **Reduced Memory Usage**: 20-50% reduction in compilation memory for object-heavy projects
- **Better Scalability**: Ability to compile larger object trees
- **Improved Performance**: Less memory allocation and copying overhead

### Long-term Benefits
- **Foundation for Advanced Optimizations**: Base for incremental compilation
- **Better Resource Utilization**: More efficient use of build machine memory
- **Enhanced Developer Experience**: Faster builds for complex projects

## Risk Mitigation

### Technical Risks
- **Binary Compatibility**: Extensive testing against existing test suite
- **Performance Regression**: Feature flag allows immediate rollback
- **Memory Corruption**: Comprehensive validation and bounds checking

### Implementation Risks
- **Complexity**: Phased approach with incremental validation
- **Testing Coverage**: Dedicated test cases for all edge cases
- **Integration Issues**: Thorough testing with existing compilation pipeline

This fix plan provides a robust, well-tested solution to enable early object deduplication while maintaining system stability and performance.